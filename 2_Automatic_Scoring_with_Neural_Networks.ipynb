{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6c5618a35e6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moavia computer\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moavia computer\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\initializers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;31m# from ALL_OBJECTS. We make no guarantees as to whether these objects will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;31m# using their correct version.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moavia computer\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\initializers\\__init__.py\u001b[0m in \u001b[0;36mpopulate_deserializable_objects\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mv2_objs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mbase_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     generic_utils.populate_dict_with_module_objects(\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mv2_objs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0minitializers_v2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import language_check\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from skll.metrics import kappa\n",
    "from bhkappa import mean_quadratic_weighted_kappa\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, LSTM, Embedding, Bidirectional, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# print('Numpy version:', np.__version__)\n",
    "# print('Pandas version:', pd.__version__)\n",
    "# print('Seaborn version:', sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Overview (Recap)\n",
    "\n",
    "### Apply grammar and spelling corrections\n",
    "\n",
    "Feature importances generated from the extra trees algorithm did not rank grammar and spelling errors highly, suggesting that perhaps the essays could be processed \"as is\". On the other hand, word embeddings rely on known words and many usage examples. Misspelled words will either be ignored if their count is below the minimum, or worse, if misspelled to an existing word with a different meaning, the vectors could change the performance of the model in a way that might not be useful. LanguageTool will be used for applying these corrections.\n",
    "\n",
    "### Clean essays\n",
    "\n",
    "Remove less helpful parts of essay texts, i.e. punctuation, pronouns and capital letters and return lemma to be processed into word embeddings. SpaCy is a simple and efficient tool for this task.\n",
    "\n",
    "### Generate word embeddings\n",
    "\n",
    "SpaCy could also be used to generate word embeddings, however its simple 'en' language model is trained on a corpus that is very different from school essays. It also uses context sensitive tensors which don't provide specific word vectors. \n",
    "\n",
    "Instead, Gensim's Word2Vec will be used to generate a model based on the all available essays, not only from the \"training_set\" which includes target scores, but also from the \"validation\" and \"test\" sets which are otherwise not useful. Finally, cleaned essays of the \"training_set\" will be passed into the word model to generate averaged 300-dimensional word vectors.\n",
    "\n",
    "### Prepare inputs\n",
    "\n",
    "There are eight topics which each have a different scale for scoring. To deal with this there are several possible approaches. First, each topic could be processed individually and the resultant kappa scores would be averaged. This approach is useful if certain features are particularly useful for grading a specific topic. On the other hand, fewer essays mean fewer examples to learn from. \n",
    "\n",
    "An alternate approach is to combine all topics by scaling the scores with a min-max scaler to a common range, e.g. 0-60. This approach increases the number of examples nearly 8-fold. \n",
    "\n",
    "### Classification or regression?\n",
    "\n",
    "Automatic scoring can now be thought of as either a classification or a regression problem. For classification the target variable needs to be one-hot encoded to the range of unique target scores and the output layer of the neural network also needs to have the same dimension. For regression, the target variable and output layer have a single dimension. Where the classification results are approximated or rounded through `softmax` activation and again when reversing the min-max scaling, the regression results are only rounded after scaling. This might be the reason regression performs slightly better on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_language(df):\n",
    "    \"\"\"\n",
    "    use language tool to correct for most spelling and grammatical errors. Also count the applied corrections. \n",
    "    Using language_check python wrapper for languagetool:\n",
    "    https://www.languagetool.org/dev\n",
    "    \"\"\"\n",
    "    tool = language_check.LanguageTool('en-US')\n",
    "\n",
    "    df['matches'] = df['essay'].apply(lambda txt: tool.check(txt))\n",
    "    df['corrections'] = df.apply(lambda l: len(l['matches']), axis=1)\n",
    "    df['corrected'] = df.apply(lambda l: language_check.correct(l['essay'], l['matches']), axis=1)\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read essays from training_set\n",
    "training_set  = pd.read_csv('training_set_rel3.tsv', sep='\\t', encoding = \"ISO-8859-1\")\\\n",
    "            .rename(columns={'essay_set': 'topic', 'domain1_score': 'target_score', 'domain2_score': 'topic2_target'})\n",
    "\n",
    "# apply spelling and grammar corrections\n",
    "training_set = correct_language(training_set)\n",
    "\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read essays from validation and test sets\n",
    "\n",
    "valid_set  = pd.read_csv('valid_set.tsv', sep='\\t', encoding = \"ISO-8859-1\")\\\n",
    "            .rename(columns={'essay_set': 'topic'})\n",
    "test_set  = pd.read_csv('test_set.tsv', sep='\\t', encoding = \"ISO-8859-1\")\\\n",
    "            .rename(columns={'essay_set': 'topic'})\n",
    "\n",
    "combo_set = pd.concat([valid_set, test_set], sort=False)\n",
    "\n",
    "# apply spelling and grammar corrections\n",
    "combo_set = correct_language(combo_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_set = pd.concat([combo_set, training_set], sort=False)\n",
    "combo_set.to_pickle('combo_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combo_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip if using language tool above\n",
    "# training_set = pd.read_pickle('training_features.pkl')\n",
    "# combo_set = pd.read_pickle('combo_set.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean training_set essays before feeding them to the Word2Vec model.\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_essays(essays, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for essay in essays.corrected:\n",
    "        if counter % 2000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(essays)))\n",
    "        counter += 1\n",
    "        essay = nlp(essay, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in essay if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup text and make sure it retains original shape\n",
    "print('Original training data shape: ', training_set['corrected'].shape)\n",
    "train_cleaned = cleanup_essays(training_set, logging=True)\n",
    "print('Cleaned up training data shape: ', train_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to preprocess text for a word2vec model\n",
    "def cleanup_essay_word2vec(essays, logging=False):\n",
    "    sentences = []\n",
    "    counter = 1\n",
    "    for essay in essays:\n",
    "        if counter % 2000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents\" % (counter, len(essays)))\n",
    "        # Disable tagger so that lemma_ of personal pronouns (I, me, etc) don't getted marked as \"-PRON-\"\n",
    "        essay = nlp(essay, disable=['tagger'])\n",
    "        # Grab lemmatized form of words and make lowercase\n",
    "        essay = \" \".join([tok.lemma_.lower() for tok in essay])\n",
    "        # Split into sentences based on punctuation\n",
    "        essay = re.split(\"[\\.?!;] \", essay)\n",
    "        # Remove commas, periods, and other punctuation (mostly commas)\n",
    "        essay = [re.sub(\"[\\.,;:!?]\", \"\", sent) for sent in essay]\n",
    "        # Split into words\n",
    "        essay = [sent.split() for sent in essay]\n",
    "        sentences += essay\n",
    "        counter += 1\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_word2vec = cleanup_essay_word2vec(combo_set['corrected'], logging=True)\n",
    "print('Cleaned up training data size (i.e. number of sentences): ', len(cleaned_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dim = 300\n",
    "print(\"Training Word2Vec model...\")\n",
    "wordvec_model = Word2Vec(cleaned_word2vec, size=text_dim, window=5, min_count=3, workers=4, sg=1)\n",
    "print(\"Word2Vec model created.\")\n",
    "print(\"%d unique words represented by %d dimensional vectors\" % (len(wordvec_model.wv.vocab), text_dim))\n",
    "wordvec_model.save('wordvec_model')\n",
    "print(\"Word2Vec model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create averaged word vectors given a cleaned text.\n",
    "def create_average_vec(essay):\n",
    "    average = np.zeros((text_dim,), dtype='float32')\n",
    "    num_words = 0.\n",
    "    for word in essay.split():\n",
    "        if word in wordvec_model.wv.vocab:\n",
    "            average = np.add(average, wordvec_model.wv[word])\n",
    "            num_words += 1.\n",
    "    if num_words != 0.:\n",
    "        average = np.divide(average, num_words)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word vectors\n",
    "cleaned_vec = np.zeros((training_set.shape[0], text_dim), dtype=\"float32\")  \n",
    "for i in range(len(train_cleaned)):\n",
    "    cleaned_vec[i] = create_average_vec(train_cleaned[i])\n",
    "\n",
    "print(\"Word vectors for all essays in the training data set are of shape:\", cleaned_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read generated features from file:\n",
    "additional_features = pd.read_pickle('training_features.pkl')\n",
    "\n",
    "# Use select features from Gini feature importances\n",
    "feature_list = [\n",
    "                'word_count',\n",
    "                'corrections',\n",
    "                'similarity',\n",
    "                'token_count',\n",
    "                'unique_token_count',\n",
    "                'nostop_count',\n",
    "                'sent_count',\n",
    "                'ner_count',\n",
    "                'comma',\n",
    "                'question',\n",
    "                'exclamation',\n",
    "                'quotation',\n",
    "                'organization',\n",
    "                'caps',\n",
    "                'person',\n",
    "                'location',\n",
    "                'money',\n",
    "                'time',\n",
    "                'date',\n",
    "                'percent',\n",
    "                'noun',\n",
    "                'adj',\n",
    "                'pron',\n",
    "                'verb',\n",
    "                'cconj',\n",
    "                'adv',\n",
    "                'det',\n",
    "                'propn',\n",
    "                'num',\n",
    "                'part',\n",
    "                'intj'\n",
    "                ]\n",
    "\n",
    "additional_features = additional_features[feature_list]\n",
    "\n",
    "stdscaler = StandardScaler()\n",
    "additional_features = stdscaler.fit_transform(additional_features)\n",
    "additional_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine topic number, target score, additional features and cleaned word vectors\n",
    "all_data = pd.concat([training_set[['topic','target_score']], pd.DataFrame(additional_features), pd.DataFrame(cleaned_vec)], axis=1)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Pass each topic individually through neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "output_dim = 1\n",
    "input_dim = all_data.shape[1]-2\n",
    "dropout = 0.2\n",
    "\n",
    "model = None\n",
    "model = Sequential()\n",
    "\n",
    "# Densely Connected Neural Network (Multi-Layer Perceptron)\n",
    "model.add(Dense(14, activation='relu', kernel_initializer='he_normal', input_dim=input_dim)) \n",
    "model.add(Dropout(dropout))\n",
    "# model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "# model.add(Dropout(dropout))\n",
    "model.add(Dense(output_dim))\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=adam, loss='mse', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each topic individually through neural network\n",
    "kappa_list = []\n",
    "weights = []\n",
    "epochs = 100\n",
    "\n",
    "for topic in range(1,9):\n",
    "    # split data\n",
    "    X = all_data[all_data.topic == topic].drop(['topic', 'target_score'], axis=1)\n",
    "    y = all_data[all_data.topic == topic].target_score.to_frame()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=26)\n",
    "    estimator = model.fit(X_train, y_train,\n",
    "#                       validation_split=0.3,\n",
    "                      epochs=epochs, batch_size=15, verbose=0)\n",
    "    # get predictions\n",
    "    y_pred = pd.DataFrame(model.predict(X_test).reshape(-1))\n",
    "    \n",
    "    # get topic kappa score\n",
    "    kappa_list.append(kappa(y_test.values, y_pred.round(0).astype(int).values, weights='quadratic'))\n",
    "\n",
    "    # get weights (number of essays)\n",
    "    weights.append(y_test.shape[0]/all_data.shape[0])    \n",
    "\n",
    "# get weighted average kappa\n",
    "qwk = mean_quadratic_weighted_kappa(kappa_list, weights=1) # weights)\n",
    "print('Combined Kappa score: {:.2f}%'.format(qwk * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again with cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "\n",
    "kappa_dict = {}\n",
    "\n",
    "for topic in range(1,9):\n",
    "    \n",
    "    model = None\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(14, input_dim=input_dim, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    X = all_data[all_data.topic == topic].drop(['topic', 'target_score'], axis=1)\n",
    "    y = all_data[all_data.topic == topic].target_score.to_frame()\n",
    "    # split data\n",
    "    kf = KFold(n_splits=5, random_state=26)\n",
    "    kappa_list = []\n",
    "    for train, test in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "        model.fit(X_train, y_train, epochs=200, batch_size=15, verbose=0) \n",
    "        y_pred = pd.DataFrame(model.predict(X_test).reshape(-1))\n",
    "        kappa_list.append(kappa(y_pred.round(0).astype(int).values, \n",
    "                        y.iloc[test].values, \n",
    "                        weights='quadratic'))\n",
    "    print(\"Kappa for topic\", topic, \": {:.3f}%\".format(np.mean(kappa_list)))\n",
    "    kappa_dict[topic] = np.mean(kappa_list)\n",
    "\n",
    "mqwk = mean_quadratic_weighted_kappa(list(kappa_dict.values()), weights=1) # weights)\n",
    "print('Combined Kappa score: {:.4f}%'.format(mqwk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted mean kappa score: 0.7844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Combine all topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach to individually fitting each topic is to combine all topics into a single model. Since each topic has a different range of scores, they essay scores need to be scaled to a common min-max range. \n",
    "\n",
    "A new dataframe with the original scores is created and passed through the train, test, split step. This is necessary to ensure proper re-scaling of target scores in cases where either the highest or lowest scores were eliminated in the *split* step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame used to pass original values through train_test_split\n",
    "scores = all_data[['topic', 'target_score']].reset_index() \n",
    "\n",
    "# Rescale target_score (essay grades) in range 0 - 60:\n",
    "scaler = MinMaxScaler((0,10))\n",
    "\n",
    "# Use this for classification:\n",
    "# ohe = OneHotEncoder(n_values=61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale and assign target variable y\n",
    "scaled = []\n",
    "for topic in range(1,9):\n",
    "    topic_scores = scores[scores['topic'] == topic]['target_score'].to_frame()\n",
    "    s = (scaler.fit_transform(topic_scores).reshape(-1))\n",
    "    scaled = np.append(scaled, s)\n",
    "    \n",
    "scores['scaled'] = scaled\n",
    "\n",
    "\"\"\"Use this for classification\"\"\"\n",
    "# Caution: using pd.get_dummies might return the wrong number of unique values\n",
    "# y = ohe.fit_transform(scores.scaled.to_frame()) # y = pd.get_dummies(scores.scaled)\n",
    "# print('Number of unique scores after scaling:', unique_scores)\n",
    "# y.shape\n",
    "\n",
    "\"\"\"Use this for regression\"\"\"\n",
    "y = scores['scaled'].to_frame()\n",
    "\n",
    "# Features\n",
    "X = all_data.drop(['topic', 'target_score'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of scaled target scores show severe class imbalance due to rescaling. Rebalancing is not applied since our metric is insensitive to class imbalance. Although both classification and regression code is shown below, the focus will be on regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score histogram\n",
    "y.hist(bins=61)\n",
    "plt.title('Histogram of scaled target scores')\n",
    "plt.xlabel('target score')\n",
    "plt.ylabel('count')\n",
    "plt.savefig('image6.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data to be split\n",
    "X_train, X_test, y_train, y_test, scores_train, scores_test = \\\n",
    "        train_test_split(\n",
    "                X, \n",
    "                y, \n",
    "                scores,\n",
    "                test_size=0.2, \n",
    "                random_state=26\n",
    "                )\n",
    "\n",
    "print('X_train size: {}'.format(X_train.shape))\n",
    "print('X_test size: {}'.format(X_test.shape))\n",
    "print('y_train size: {}'.format(y_train.shape))\n",
    "print('y_test size: {}'.format(y_test.shape))\n",
    "print('scores_train size: {}'.format(scores_train.shape))\n",
    "print('scores_test size: {}'.format(scores_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = y.shape[1]\n",
    "input_dim = X.shape[1]\n",
    "dropout = 0.2\n",
    "\n",
    "def build_model(architecture='mlp'):\n",
    "    model = Sequential()\n",
    "    if architecture == 'mlp':\n",
    "        # Densely Connected Neural Network (Multi-Layer Perceptron)\n",
    "        model.add(Dense(14, activation='relu', kernel_initializer='he_normal', input_dim=input_dim)) \n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "# Classification:\n",
    "#         model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "# Regression:\n",
    "        model.add(Dense(output_dim))\n",
    "    elif architecture == 'cnn':\n",
    "        # 1-D Convolutional Neural Network\n",
    "        inputs = Input(shape=(input_dim,1))\n",
    "\n",
    "        x = Conv1D(64, 3, strides=1, padding='same', activation='relu')(inputs)\n",
    "\n",
    "        #Cuts the size of the output in half, maxing over every 2 inputs\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "        x = Conv1D(128, 3, strides=1, padding='same', activation='relu')(x)\n",
    "        x = GlobalMaxPooling1D()(x) \n",
    "        outputs = Dense(output_dim, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='CNN')\n",
    "    elif architecture == 'lstm':\n",
    "        # LSTM network\n",
    "        inputs = Input(shape=(input_dim,1))\n",
    "\n",
    "        x = Bidirectional(LSTM(64, return_sequences=True),\n",
    "                          merge_mode='concat')(inputs)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Flatten()(x)\n",
    "        outputs = Dense(output_dim, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='LSTM')\n",
    "    else:\n",
    "        print('Error: Model type not found.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Of the three model architectures the multi-layer perceptron, or 'mlp', consistently produced the highest kappa scores in the shortest amount of calculation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keras model\n",
    "model = None\n",
    "# Using MLP in kernel for speed\n",
    "model = build_model('mlp')\n",
    "# model = build_model('cnn')\n",
    "# model = build_model('lstm')\n",
    "\n",
    "# If the model is a CNN then expand the dimensions of the training data\n",
    "if model.name == \"CNN\" or model.name == \"LSTM\":\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    print('Text train shape: ', X_train.shape)\n",
    "    print('Text test shape: ', X_test.shape)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# Optimizer\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Classification:\n",
    "# model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Regression:\n",
    "model.compile(optimizer=adam, loss='mse', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Fit the model to the training data\n",
    "estimator = model.fit(X_train, y_train,\n",
    "#                       validation_split=0.3,\n",
    "                      epochs=epochs, batch_size=15, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classification: Test set predictions \"\"\"\n",
    "# y_proba = model.predict(X_test)\n",
    "# y_pred = pd.Series(y_proba.argmax(axis=-1))\n",
    "\n",
    "# Classification, if using pd.get_dummies:\n",
    "# y_t = pd.Series(y_test.columns[np.where(y_test != 0)[1]])\n",
    "\n",
    "# Classification, if using OneHotEncoder:\n",
    "# y_t = csr_matrix.argmax(y_test, axis=1)\n",
    "# y_t = pd.Series(np.squeeze(np.asarray(y_t)))\n",
    "\n",
    "# Regression, use 'y_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regression\n",
    "True y, not needed for kappa metric because already passed with train, test, split\n",
    "Used to verify reverse scaling\n",
    "\"\"\"\n",
    "y_pred = pd.DataFrame(model.predict(X_test).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Reverse scaling back to original target score scales\"\"\"\n",
    "\n",
    "# Merge results\n",
    "results = scores_test.reset_index(drop=True)\\\n",
    "                    .join(y_pred)\\\n",
    "                    .rename(columns={0:'y_pred'})\\\n",
    "                    .sort_values(by='topic')\\\n",
    "                    .reset_index(drop=True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms of true and predicted target scores show generally a good agreement with the distribution trend. The plots are somewhat misleading since the scaled target scores have the same number of unique values before and after scaling. Since the Kappa metric calculates the distance, the spread of predicted values is not as worrisome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_number = 0\n",
    "fig, ax = plt.subplots(4,2, figsize=(9,9), sharey=False)\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        topic_number += 1\n",
    "        results[results['topic'] == topic_number]\\\n",
    "            [['scaled', 'y_pred']]\\\n",
    "            .plot.hist(histtype='step', bins=20, ax=ax[i, j], rot=0)\n",
    "        ax[i,j].set_title('Topic %i' % topic_number)\n",
    "ax[3,0].locator_params(nbins=10)\n",
    "ax[3,1].locator_params(nbins=10)\n",
    "plt.suptitle('Scaled prediction errors')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction errors are greatest at the highest and lowest scores. Typically, the extreme scores have low representation and therefore the highest expected variance, therefore this is, once again, less worrisome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_number = 0\n",
    "fig, ax = plt.subplots(4,2, figsize=(9,9), sharey=False)\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        topic_number += 1\n",
    "        results[results['topic'] == topic_number]\\\n",
    "            .groupby('target_score')['diff']\\\n",
    "            .median()\\\n",
    "            .plot.bar(ax=ax[i, j], rot=0)\n",
    "        ax[i,j].set_title('Topic %i' % topic_number)\n",
    "ax[3,0].locator_params(nbins=10)\n",
    "ax[3,1].locator_params(nbins=10)\n",
    "plt.suptitle('Scaled prediction errors')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create list of tuples with min/max target scores sorted by topic number.\n",
    " Performed here on results in case min/max values didn't pass through train_test_split\"\"\"\n",
    "\n",
    "score_df = results.groupby('topic')['target_score'].agg(['min', 'max'])  \n",
    "score_ranges = list(zip(score_df['min'], score_df['max'])) \n",
    "\n",
    "\"\"\"Shrink back to original range by topic number:\"\"\"\n",
    "y_p_df = pd.Series()\n",
    "y_t_df = pd.Series()\n",
    "\n",
    "for topic in range(1,9):\n",
    "    scaler = MinMaxScaler(score_ranges[topic-1])\n",
    "    scaled_pred = results[results.topic == topic]['y_pred'].to_frame()\n",
    "    y_pred_shrunk = scaler.fit_transform(scaled_pred).round(0).astype('int')\n",
    "    scaled_true = results[results.topic == topic]['scaled'].to_frame()\n",
    "    y_true_shrunk = scaler.fit_transform(scaled_true).round(0).astype('int')\n",
    "    y_p_df = y_p_df.append(pd.Series(np.squeeze(np.asarray(y_pred_shrunk))), ignore_index=True)\n",
    "    y_t_df = y_t_df.append(pd.Series(np.squeeze(np.asarray(y_true_shrunk))), ignore_index=True)\n",
    "    \n",
    "# Append to results df\n",
    "results['pred'] = y_p_df\n",
    "results['y_true'] = y_t_df\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The close agreement between true and predicted essay scores becomes apparent after re-scaling to their original ranges and plotting on a log scale by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score histogram\n",
    "\n",
    "results[['pred', 'y_true']].plot.hist(histtype='step', bins=20, logy=True)\n",
    "plt.title('Histogram of target scores')\n",
    "plt.xlabel('target score')\n",
    "plt.ylabel('log count')\n",
    "plt.savefig('image8.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification accuracy:\n",
    "# print(\"Training accuracy: %.2f%% / Validation accuracy: %.2f%%\" % \n",
    "#       (100*estimator.history['acc'][-1], 100*estimator.history['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on kappa scores\n",
    "\n",
    "Some previous studies apply the kappa metric directly to the complete set of essays. Due to the differences in scale, essay topics (sets) that have a narrow scoring range will end up with significantly smaller weighted distances and thus artificially high kappa scores. Kappa scores of up to 94.5% have been reported with this metric.\n",
    "\n",
    "Other studies calculate the kappa score for each topic individually and then report the geometric mean.\n",
    "\n",
    "The method most likely used in the original Kaggle competition (there is some doubt whether the published code matches the one actually used), uses the quadratic weighted kappa for each set individually and then calculates the mean of the quadratic weighted kappas after applying Fisher's r-to-z transform, which is approximately a variance-stabilizing transformation. Each set is weighted by a factor of one, except for topic 2 which has two domain raters which are each weighted 0.5. It is not clear why the scores are not weighted by the number of essays in each set, then again, the actual formulas used in the competition have not been published.\n",
    "\n",
    "The kappa scores from the original competition were based on comparison to unpublished test data scores. Since all published work on this data set only used the training data set, kappas cannot be compared directly to competition results.\n",
    "\n",
    "Finally, most published work used a subset of essay sets (topics). Omitting topics is likely going to change kappa scores for better or worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = kappa(results.pred, results.target_score, weights='quadratic')\n",
    "print('Combined essay kappa score: {:.4f}'.format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined essay topics kappa score is much higher than that obtained from individual topics, however, as noted above, this is deceptive, as shown in a recalculation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwk = []\n",
    "# weights = []\n",
    "for topic in range(1,9):\n",
    "    qwk.append(\n",
    "            kappa(results[results.topic == topic]['target_score'], \n",
    "                  results[results.topic == topic]['pred'],\n",
    "                    weights='quadratic'))\n",
    "#     weights.append(len(results[results.topic==topic])/X_test.shape[0])    \n",
    "mqwk = mean_quadratic_weighted_kappa(qwk, weights=1)\n",
    "print('Weighted by topic Kappa score: {:.2f}%'.format(mqwk * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kappa for two human raters\n",
    "qwk = []\n",
    "# weights = []\n",
    "for topic in range(1,9):\n",
    "    qwk.append(\n",
    "            kappa(training_set[training_set.topic == topic]['rater1_domain1'], \n",
    "                  training_set[training_set.topic == topic]['rater2_domain1'],\n",
    "                    weights='quadratic'))\n",
    "#     weights.append(len(results[results.topic==topic])/X_test.shape[0])    \n",
    "mqwk = mean_quadratic_weighted_kappa(qwk, weights=1)\n",
    "print('Weighted by topic Kappa score: {:.4f}'.format(mqwk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_number = 0\n",
    "fig, ax = plt.subplots(4,2, figsize=(8,8), sharey=False)\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        topic_number += 1\n",
    "        results[results['topic'] == topic_number]\\\n",
    "            .groupby('y_true')['y_true']\\\n",
    "            .agg('count')\\\n",
    "            .plot.bar(ax=ax[i, j], rot=0, fill=False, ec='b', label='actual')\n",
    "        results[results['topic'] == topic_number]\\\n",
    "            .groupby('pred')['pred']\\\n",
    "            .agg('count')\\\n",
    "            .plot.bar(ax=ax[i, j], rot=0, fill=False, ec='r', label='prediction')\n",
    "        ax[i,j].set_title('Topic %i' % topic_number)\n",
    "ax[3,0].locator_params(nbins=10)\n",
    "ax[3,1].locator_params(nbins=10)\n",
    "plt.suptitle('Histograms of predicted essay scores')\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.05))\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use this for plotting regression mean-squared error\n",
    "Validation split must be enabled during model fit.\n",
    "\"\"\"\n",
    "# Plot model mse over epochs\n",
    "sns.reset_orig()   # Reset seaborn settings to get rid of black background\n",
    "plt.plot(estimator.history['mean_squared_error'])\n",
    "plt.plot(estimator.history['val_mean_squared_error'])\n",
    "plt.title('model MSE')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "# plt.ylim(0,300)\n",
    "plt.show()\n",
    "\n",
    "# Plot model loss over epochs\n",
    "plt.plot(estimator.history['loss'])\n",
    "plt.plot(estimator.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most significant improvement occurs in the first 10 or 20 epochs, however the model continues to improve slightly with additional epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use this for plotting classification accuracy\"\"\"\n",
    "# # Plot model accuracy over epochs\n",
    "# sns.reset_orig()   # Reset seaborn settings to get rid of black background\n",
    "# plt.plot(estimator.history['acc'])\n",
    "# plt.plot(estimator.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'valid'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot model loss over epochs\n",
    "# plt.plot(estimator.history['loss'])\n",
    "# plt.plot(estimator.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'valid'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for scaling errors\n",
    "errors = len(results.query('y_true != target_score')[['topic', 'target_score', 'y_true']])\n",
    "\n",
    "print('{:.1f}% of target scores did not revert back to their original value.'.format(errors/results.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Adding Word2Vec word embeddings to the feature set and applying a neural network model produces superior results compared to traditional machine learning algorithms. \n",
    "\n",
    "Of all the model architectures attempted, a multi-layer perceptron with surprisingly few nodes in the hidden layer returned the highest Kappa score. Additional hidden layers did not improve the model.\n",
    "\n",
    "Automatic scoring can be framed as both a classification or a regression problem. The highest target metrics (mean quadratic weighted kappa) were found using regression analysis.\n",
    "\n",
    "Comparisons to the original Kaggle competition and later attempts are often unfair due to differences in data sets and methods of calculating kappa. The results obtained in this notebook are among the highest, if not the highest currently published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "Additional optimization is definitely possibly by tweaking hyperparameters and calculating more epochs. Additional features could be extracted and added to the feature set. While this would almost certainly result in higher kappa scores, the additional gain wouldn't outweigh the benefit. Perhaps the biggest obstacle is the limited data. If some day scores for the validation and test data sets are released, it would be worthwhile revisiting the problem set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
