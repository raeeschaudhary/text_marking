{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Scoring with Machine Learning\n",
    "\n",
    "This notebook explores classic machine learning algorithms with vectorized features from the student essays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and setup notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi']= 100\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression\n",
    "\n",
    "# kappa metric for measuring agreement of automatic to human scores\n",
    "from skll.metrics import kappa\n",
    "from bhkappa import mean_quadratic_weighted_kappa\n",
    "\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "# Setup Pandas\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_spacy.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d6d7dd6f5a49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read essay data processed in previous notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraining_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_spacy.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\moavia computer\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \"\"\"\n\u001b[0;32m    172\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moavia computer\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_spacy.pkl'"
     ]
    }
   ],
   "source": [
    "# Read essay data processed in previous notebook\n",
    "training_set = pd.read_pickle('training_spacy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[['lemma', 'pos', 'ner']].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vectorized features from processed essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document similarity metric is available from *SpaCy*. In order to make use of it for essay scoring, we need to define a reference. Choosing an average, middle-scoring or aggregate essay would leave the sign of the difference undetermined: If the similarity is worse, does that mean the essay is better or worse? Choosing a low scoring essay would theoretically work, however many of the low scoring essays are very short and are full of spelling and grammatical errors. A high scoring essay would be best, though there is another point to consider. When an essay is written in a unique style, how will it compare? Since there are relatively few high scoring essays, the selection was performed manually and arbitrarily under consideration of a \"representative style\". \n",
    "\n",
    "The selection process remains highly subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Choose arbitrary essay from highest available target_score for each topic.\n",
    "all other essays will be compared to these. \n",
    "The uncorrected essays will be used since the reference essays should have fewer errors.\n",
    "\"\"\"\n",
    "reference_essays = {1: 161, 2: 3022, 3: 5263, 4: 5341, 5: 7209, 6: 8896, 7: 11796, 8: 12340} # topic: essay_id\n",
    "\n",
    "references = {}\n",
    "\n",
    "t0 = datetime.now()\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "stop_words = set(STOP_WORDS)\n",
    "\n",
    "# generate nlp object for reference essays:\n",
    "for topic, index in reference_essays.items():\n",
    "    references[topic] = nlp(training_set.iloc[index]['essay'])\n",
    "\n",
    "# generate document similarity for each essay compared to topic reference\n",
    "training_set['similarity'] = training_set.apply(lambda row: nlp(row['essay']).similarity(references[row['topic']]), axis=1)\n",
    "\n",
    "t1 = datetime.now()\n",
    "print('Processing time: {}'.format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot document similarity vs target score for each topic\n",
    "topic_number = 0\n",
    "fig, ax = plt.subplots(4,2, figsize=(7,10))\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        topic_number += 1\n",
    "        sns.regplot(x='target_score', y='similarity', data=training_set[training_set['topic'] == topic_number], ax=ax[i,j])\n",
    "        ax[i,j].set_title('Topic %i' % topic_number)\n",
    "ax[3,0].locator_params(nbins=10)\n",
    "ax[3,1].locator_params(nbins=10)\n",
    "plt.suptitle('Document similarity by topic')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig('image5.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document similarity may prove to be ineffective for persuasive/narritive essays. The example below shows the highest scored essay for topic 8. The author uses a unique creative style which is unlikely to be replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.iloc[12340]['essay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count various features\n",
    "\n",
    "t0 = datetime.now()\n",
    "\n",
    "training_set['token_count'] = training_set.apply(lambda x: len(x['tokens']), axis=1)\n",
    "training_set['unique_token_count'] = training_set.apply(lambda x: len(set(x['tokens'])), axis=1)\n",
    "training_set['nostop_count'] = training_set \\\n",
    "            .apply(lambda x: len([token for token in x['tokens'] if token not in stop_words]), axis=1)\n",
    "training_set['sent_count'] = training_set.apply(lambda x: len(x['sents']), axis=1)\n",
    "training_set['ner_count'] = training_set.apply(lambda x: len(x['ner']), axis=1)\n",
    "training_set['comma'] = training_set.apply(lambda x: x['corrected'].count(','), axis=1)\n",
    "training_set['question'] = training_set.apply(lambda x: x['corrected'].count('?'), axis=1)\n",
    "training_set['exclamation'] = training_set.apply(lambda x: x['corrected'].count('!'), axis=1)\n",
    "training_set['quotation'] = training_set.apply(lambda x: x['corrected'].count('\"') + x['corrected'].count(\"'\"), axis=1)\n",
    "training_set['organization'] = training_set.apply(lambda x: x['corrected'].count(r'@ORGANIZATION'), axis=1)\n",
    "training_set['caps'] = training_set.apply(lambda x: x['corrected'].count(r'@CAPS'), axis=1)\n",
    "training_set['person'] = training_set.apply(lambda x: x['corrected'].count(r'@PERSON'), axis=1)\n",
    "training_set['location'] = training_set.apply(lambda x: x['corrected'].count(r'@LOCATION'), axis=1)\n",
    "training_set['money'] = training_set.apply(lambda x: x['corrected'].count(r'@MONEY'), axis=1)\n",
    "training_set['time'] = training_set.apply(lambda x: x['corrected'].count(r'@TIME'), axis=1)\n",
    "training_set['date'] = training_set.apply(lambda x: x['corrected'].count(r'@DATE'), axis=1)\n",
    "training_set['percent'] = training_set.apply(lambda x: x['corrected'].count(r'@PERCENT'), axis=1)\n",
    "training_set['noun'] = training_set.apply(lambda x: x['pos'].count('NOUN'), axis=1)\n",
    "training_set['adj'] = training_set.apply(lambda x: x['pos'].count('ADJ'), axis=1)\n",
    "training_set['pron'] = training_set.apply(lambda x: x['pos'].count('PRON'), axis=1)\n",
    "training_set['verb'] = training_set.apply(lambda x: x['pos'].count('VERB'), axis=1)\n",
    "training_set['noun'] = training_set.apply(lambda x: x['pos'].count('NOUN'), axis=1)\n",
    "training_set['cconj'] = training_set.apply(lambda x: x['pos'].count('CCONJ'), axis=1)\n",
    "training_set['adv'] = training_set.apply(lambda x: x['pos'].count('ADV'), axis=1)\n",
    "training_set['det'] = training_set.apply(lambda x: x['pos'].count('DET'), axis=1)\n",
    "training_set['propn'] = training_set.apply(lambda x: x['pos'].count('PROPN'), axis=1)\n",
    "training_set['num'] = training_set.apply(lambda x: x['pos'].count('NUM'), axis=1)\n",
    "training_set['part'] = training_set.apply(lambda x: x['pos'].count('PART'), axis=1)\n",
    "training_set['intj'] = training_set.apply(lambda x: x['pos'].count('INTJ'), axis=1)\n",
    "\n",
    "t1 = datetime.now()\n",
    "print('Processing time: {}'.format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "training_set.to_pickle('training_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_pickle('training_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the generated features are correlated with essay length. Collinearity is potentially an issue here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation of essay-length related features\n",
    "usecols = ['word_count', 'token_count', 'unique_token_count', 'nostop_count', 'sent_count']\n",
    "g = sns.pairplot(training_set[training_set.topic == 4], hue='target_score', vars=usecols, plot_kws={\"s\": 20}, palette=\"bright\")\n",
    "g.fig.subplots_adjust(top=.93)\n",
    "g.fig.suptitle('Pairplots of select features', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incomplete columns are not used for modeling and can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate feature selection performed on the vectorized data shows few differences in the 10 best features by topic number. It is not surprising that `similarity` has little influence on `target_score` in topic 4 since there are only four unique scores and the the similarity by score plot above shows high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting k best features: Some features omitted due to high correlation\n",
    "\n",
    "predictors = [  \n",
    "#                 'word_count',\n",
    "                'corrections',\n",
    "                'similarity',\n",
    "#                 'token_count',\n",
    "                'unique_token_count',\n",
    "#                 'nostop_count',\n",
    "                'sent_count',\n",
    "                'ner_count',\n",
    "                'comma',\n",
    "                'question',\n",
    "                'exclamation',\n",
    "                'quotation',\n",
    "                'organization',\n",
    "                'caps',\n",
    "                'person',\n",
    "                'location',\n",
    "                'money',\n",
    "                'time',\n",
    "                'date',\n",
    "                'percent',\n",
    "                'noun',\n",
    "                'adj',\n",
    "                'pron',\n",
    "                'verb',\n",
    "                'cconj',\n",
    "                'adv',\n",
    "                'det',\n",
    "                'propn',\n",
    "                'num',\n",
    "                'part',\n",
    "                'intj'\n",
    "                ]\n",
    "\n",
    "# Create and fit selector\n",
    "selector = SelectKBest(f_regression, k=10) # f_classif, chi2, f_regression, mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Create empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for topic in range(1, 9):\n",
    "    kpredictors = []\n",
    "    \n",
    "    # test for division by zero errors due to insufficient data:\n",
    "    for p in predictors:\n",
    "        if np.std(training_set[training_set.topic == topic][p], axis=0) != 0:\n",
    "            kpredictors.append(p)\n",
    "            \n",
    "    # select k best for each topic:\n",
    "    X = training_set[training_set.topic == topic][kpredictors]\n",
    "    y = training_set[training_set.topic == topic].target_score\n",
    "    \n",
    "    selector.fit(X, y)\n",
    "\n",
    "    # Get idxs of columns to keep\n",
    "    mask = selector.get_support(indices=True)\n",
    "\n",
    "    selected_features = training_set[training_set.topic == topic][predictors].columns[mask]\n",
    "    df[\"Topic \" + str(topic)] = selected_features\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the regression pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df, topic, features, model):\n",
    "    \"\"\"Regression pipeline with kappa evaluation\"\"\"\n",
    "\n",
    "    X = df[df['topic'] == topic][features]\n",
    "    y = df[df['topic'] == topic]['target_score'].astype(np.float64)\n",
    "    # token_ct = X.token_count\n",
    "    # X = X.div(token_ct, axis=0)\n",
    "    # X['token_count'] = X['token_count'].mul(token_ct, axis=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=26)\n",
    "    \n",
    "    pipeline = Pipeline(model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    return kappa(y_pred, y_test, weights='quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative feature selection strategy is to use **L1** regularization to limit the influence of less important features. This is implemented below in the ElasticNet regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [  \n",
    "                'word_count',\n",
    "                'corrections',\n",
    "                'similarity',\n",
    "                'token_count',\n",
    "                'unique_token_count',\n",
    "                'nostop_count',\n",
    "                'sent_count',\n",
    "                'ner_count',\n",
    "                'comma',\n",
    "                'question',\n",
    "                'exclamation',\n",
    "                'quotation',\n",
    "                'organization',\n",
    "                'caps',\n",
    "                'person',\n",
    "                'location',\n",
    "                'money',\n",
    "                'time',\n",
    "                'date',\n",
    "                'percent',\n",
    "                'noun',\n",
    "                'adj',\n",
    "                'pron',\n",
    "                'verb',\n",
    "                'cconj',\n",
    "                'adv',\n",
    "                'det',\n",
    "                'propn',\n",
    "                'num',\n",
    "                'part',\n",
    "                'intj'\n",
    "                ]\n",
    "\n",
    "# feature selection\n",
    "# fvalue_selector = SelectKBest(score_func=f_regression, k=10)\n",
    "\n",
    "# for use in pipeline\n",
    "models = [\n",
    "            [('scaler', StandardScaler()),('linearSVC', LinearSVC(C=0.01))] ,\n",
    "            [('scaler', StandardScaler()),('lm', LinearRegression())], \n",
    "            [('rf', RandomForestRegressor(random_state=26))],  \n",
    "            [('en', ElasticNet(l1_ratio=0.01, alpha=0.1, max_iter=100000, random_state=26))] \n",
    "        ]\n",
    "\n",
    "for steps in models:\n",
    "    kappas = []\n",
    "    weights = []\n",
    "    for topic in range(1,9):\n",
    "        kappas.append(evaluate(training_set, topic, predictors, steps))\n",
    "        weights.append(len(training_set[training_set.topic==topic]))\n",
    "\n",
    "    mqwk = mean_quadratic_weighted_kappa(kappas, weights=weights)\n",
    "    print(steps)\n",
    "    print('Weighted by topic Kappa score: {:.4f}'.format(mqwk))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the four models on which the data was evaluated, the three regression models returned very similar mean weighted kappa scores and the simple linear regression model slightly outperformed the others. The support vector classifier performed poorly. \n",
    "\n",
    "Can we improve on the hyperparameters for ElasticNet by running GridSearchCV on each topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet with GridSearchCV for each individual topic\n",
    "\n",
    "def en_evaluate(df, topic, features):\n",
    "    # Regression pipeline with kappa evaluation\n",
    "    paramgrid = {'l1_ratio': [.01, .1, .3, .5, .7, .99], 'alpha': [0.001, 0.01, 0.1, 1]}\n",
    "    X = df[df['topic'] == topic][features]\n",
    "    y = df[df['topic'] == topic]['target_score'].astype(np.float64)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=26)\n",
    "    \n",
    "    gs = GridSearchCV(ElasticNet(max_iter=100000, random_state=26),\n",
    "                      param_grid=paramgrid,\n",
    "                      cv=5)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Topic', topic, 'best parameters:', gs.best_params_)\n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    return kappa(y_pred, y_test, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappas = []\n",
    "weights = []\n",
    "for topic in range(1,9):\n",
    "    kappas.append(en_evaluate(training_set, topic, predictors))\n",
    "    weights.append(len(training_set[training_set.topic==topic]))\n",
    "    \n",
    "mqwk = mean_quadratic_weighted_kappa(kappas, weights=weights)\n",
    "print('Weighted by topic Kappa score: {:.4f}'.format(mqwk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low `l1_ratio` implies Ridge regression with **l2** regularization. The weighted Kappa score did not improve measurably when hyperparameters are tuned for each topic, including cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual topic kappa scores\n",
    "kappas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final approach for feature selection is to extract the Gini-importances of random forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set[predictors]\n",
    "y = training_set['target_score'].astype(np.float64)\n",
    "\n",
    "\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=26)\n",
    "\n",
    "forest.fit(X, y)\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "# plot feature importances\n",
    "\n",
    "features = pd.DataFrame({'feature_name': X.columns, 'importance': forest.feature_importances_, 'std': std})\n",
    "features.sort_values('importance')\\\n",
    "        .plot.barh(x='feature_name', y='importance', xerr='std', legend=False)\n",
    "plt.title('Gini importances of forest features')\n",
    "plt.xlabel('Gini-importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best k features\n",
    "k = 15\n",
    "top_features = features.sort_values('importance', ascending=False)['feature_name'].tolist()[:k]\n",
    "\n",
    "# Linear regression with top k features\n",
    "kappas = []\n",
    "weights = []\n",
    "steps = [('scaler', StandardScaler()),('lm', LinearRegression())]\n",
    "for topic in range(1,9):\n",
    "    kappas.append(evaluate(training_set, topic, top_features, steps))\n",
    "    weights.append(len(training_set[training_set.topic==topic]))\n",
    "\n",
    "mqwk = mean_quadratic_weighted_kappa(kappas, weights=weights)\n",
    "print('Weighted by topic Kappa score: {:.4f}'.format(mqwk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kappa scores increase with increasing number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown earlier and in the correlation matrix below, some features are highly correlated. This can lead to problems if there are insufficient observations to explain the differences between features. Signs of potential collinearity problems could be poor generalization of the model. In this case, the Kappa scores did not change dramatically when using training and test data or when applying cross-validation.\n",
    "\n",
    "Models that apply feature selection might automatically remove some highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of correlating features\n",
    "corr = training_set[predictors].corr() # default: Pearson\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "g = sns.heatmap(corr, mask=mask, cmap='Spectral', center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized essays re-joined (list to essay)\n",
    "training_set['l_essay'] = training_set['lemma'].apply(' '.join)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=.2, \n",
    "                             min_df=3, \n",
    "                             max_features=2000,\n",
    "                             stop_words=STOP_WORDS) # default: binary=False\n",
    "tfidf_matrix = vectorizer.fit_transform(training_set.l_essay) # using lemmatized essays\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[predictors].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine previous predictors with TF-IDF matrix\n",
    "combined_dense = pd.concat([pd.DataFrame(tfidf_matrix.todense()), \n",
    "                            training_set[predictors], \n",
    "                            training_set['topic'], \n",
    "                            training_set['target_score']], axis=1)\n",
    "combined_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet with GridSearchCV for each individual topic\n",
    "\n",
    "def tf_evaluate(df, topic):\n",
    "    # Regression pipeline with kappa evaluation\n",
    "    paramgrid = {'l1_ratio': [.01, .1, .5, .9], 'alpha': [0.01, .1, 1]}\n",
    "    X = df[df['topic'] == topic].drop(['topic', 'target_score'], axis=1)\n",
    "    y = df[df['topic'] == topic]['target_score'].astype(np.float64)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=26)\n",
    "    \n",
    "    gs = GridSearchCV(ElasticNet(max_iter=100000, random_state=26),\n",
    "                      param_grid=paramgrid,\n",
    "                      cv=5)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Topic', topic, 'best parameters:', gs.best_params_)\n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    return kappa(y_pred, y_test, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet with GridSearchCV for each individual topic\n",
    "\n",
    "kappas = []\n",
    "weights = []\n",
    "for topic in range(1,9):\n",
    "    kappas.append(tf_evaluate(combined_dense, topic))\n",
    "    weights.append(len(training_set[training_set.topic==topic]))\n",
    "    \n",
    "mqwk = mean_quadratic_weighted_kappa(kappas, weights=weights)\n",
    "print('Weighted by topic Kappa score: {:.4f}'.format(mqwk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding TF-IDF features only marginally improved the kappa score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
