{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Topic Modeling\n",
    "\n",
    "This section will cover:\n",
    "\n",
    "* initial exploratory analysis\n",
    "* grammar and spelling correction \n",
    "* feature extraction with SpaCy \n",
    "* topic modeling with latent Dirichlet allocation (LDA).\n",
    "* automatic scoring with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (c:\\users\\moavia computer\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-496e6a5bb50d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (c:\\users\\moavia computer\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "\n",
    "import language_check\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pyLDAvis\n",
    "from pyLDAvis.sklearn import prepare\n",
    "\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "# Setup Pandas\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "# plt.rcParams['figure.figsize'] = [8, 5]\n",
    "plt.rcParams['figure.dpi']= 100\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set  = pd.read_csv('training_set_rel3.tsv', sep='\\t', encoding = \"ISO-8859-1\")\\\n",
    "            .rename(columns={'essay_set': 'topic', 'domain1_score': 'target_score', 'domain2_score': 'topic2_target'})\n",
    "training_set.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.groupby('topic').agg('count').plot.bar(y='essay', rot=0, legend=False)\n",
    "plt.title('Essay count by topic #')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since topic 8 has the fewest essays and the most distinct scores, it might prove to be the most challenging topic to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count characters and words for each essay\n",
    "training_set['word_count'] = training_set['essay'].str.strip().str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.hist(column='word_count', by='topic', bins=25, sharey=True, sharex=True, layout=(2, 4), figsize=(7,4), rot=0) \n",
    "plt.suptitle('Word count by topic #')\n",
    "plt.xlabel('Number of words')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.groupby(['topic'])['target_score'].agg(['min','max','count','nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_number = 0\n",
    "fig, ax = plt.subplots(4,2, figsize=(8,10))\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        topic_number += 1\n",
    "        sns.violinplot(x='target_score', y='word_count', data=training_set[training_set['topic'] == topic_number], ax=ax[i,j])\n",
    "        ax[i,j].set_title('Topic %i' % topic_number)\n",
    "ax[3,0].locator_params(nbins=10)\n",
    "ax[3,1].locator_params(nbins=10)\n",
    "plt.suptitle('Word count by score')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a reasonable correlation between word count and score for all but topic 8 where the word count apparently reaches a maximum at the upper third of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_number = 0\n",
    "fig, ax = plt.subplots(4,2, figsize=(9,9), sharey=False)\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        topic_number += 1\n",
    "        training_set[training_set['topic'] == topic_number]\\\n",
    "            .groupby('target_score')['essay_id']\\\n",
    "            .agg('count')\\\n",
    "            .plot.bar(ax=ax[i, j], rot=0)\n",
    "        ax[i,j].set_title('Topic %i' % topic_number)\n",
    "ax[3,0].locator_params(nbins=10)\n",
    "ax[3,1].locator_params(nbins=10)\n",
    "plt.suptitle('Histograms of essay scores')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many scores are underrepresented. Classification could be difficult without rebalancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essay processing:\n",
    "\n",
    "1. Language correction with languagetool (add number of corrections as feature)\n",
    "2. Sentence tokenization with Spacy\n",
    "3. Sentence count and length\n",
    "4. Word tokenize with Spacy\n",
    "5. Word token classification (punctuation, stop words and anonymized entities, pos, ent)\n",
    "6. Document similarity (based on arbitrary high scoring essay for each topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar and spelling\n",
    "\n",
    "As to be expected with student essays, many essays exhibit grammar and spelling errors. \n",
    "\n",
    "Parts-of-speech (POS) and named-entity-recognition (NER) is hampered in part by the lack of consitent spelling and punctuation. Therefore, the essays will be corrected using languagetool and the nlp parsing will be performed with Spacy on the corrected essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Some people are still using Myspoce instead of facdbook'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textblob is an open source nlp package and something to keep an eye on. Unlike Spacy, textblob includes a statistics based spell checker. It only claims 70% accuracy, and in my testing it didn't perform well.\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "data = TextBlob(text)\n",
    "print (data.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languagetool.org has a python wrapper for spelling and grammatical errors at \n",
    "https://github.com/myint/language-check\n",
    "\n",
    "It appears to work quite well, although it requires intermediate storage of a list of errors ('matches')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_check.LanguageTool('en-US')\n",
    "\n",
    "matches = tool.check(text)\n",
    "language_check.correct(text, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = training_set.essay[1871]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "matches = tool.check(text)\n",
    "language_check.correct(text, matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the last sentence, languagetool did not correct `\",therefor\"`. Nonetheless, it should be good enough to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use language tool to correct for most spelling and grammatical errors. Also count the applied corrections. \n",
    "Using language_check python wrapper for languagetool:\n",
    "https://www.languagetool.org/dev\n",
    "\"\"\"\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "\n",
    "t0 = datetime.now()\n",
    "\n",
    "training_set['matches'] = training_set['essay'].apply(lambda txt: tool.check(txt))\n",
    "training_set['corrections'] = training_set.apply(lambda l: len(l['matches']), axis=1)\n",
    "training_set['corrected'] = training_set.apply(lambda l: language_check.correct(l['essay'], l['matches']), axis=1)\n",
    "\n",
    "t1 = datetime.now()\n",
    "print('Processing time: {}'.format(t1 - t0))\n",
    "\n",
    "# save work\n",
    "training_set.to_pickle('training_corr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a very special example of poor writing skills (or perhaps a digitization error?). None of the spell checkers I tried were able to make much sense out of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Original:')\n",
    "print(training_set.essay[18])\n",
    "print('Corrected with languagetool:')\n",
    "print(training_set.corrected[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with SpaCy\n",
    "\n",
    "Although much of the analysis could be performed with other NLP packages, SpaCy was chosen due to its combination of speed and simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_pickle('training_corr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "ner = []\n",
    "\n",
    "stop_words = set(STOP_WORDS)\n",
    "stop_words.update(punctuation) # remove it if you need punctuation \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "t0 = datetime.now()\n",
    "\n",
    "# suppress numpy warnings\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "for essay in nlp.pipe(training_set['corrected'], batch_size=100, n_threads=3):\n",
    "    if essay.is_parsed:\n",
    "        tokens.append([e.text for e in essay])\n",
    "        sents.append([sent.string.strip() for sent in essay.sents])\n",
    "        pos.append([e.pos_ for e in essay])\n",
    "        ner.append([e.text for e in essay.ents])\n",
    "        lemma.append([n.lemma_ for n in essay])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "        sents.append(None)\n",
    "        ner.append(None)\n",
    "\n",
    "training_set['tokens'] = tokens\n",
    "training_set['lemma'] = lemma\n",
    "training_set['pos'] = pos\n",
    "training_set['sents'] = sents\n",
    "training_set['ner'] = ner\n",
    "\n",
    "t1 = datetime.now()\n",
    "print('Processing time: {}'.format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.to_pickle('training_spacy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_pickle('training_spacy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[['tokens', 'pos', 'sents', 'ner']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation, or **LDA**, uses probabilities to allocate any number of documents to a pre-defined number of topics. A very good explanation is given here:\n",
    "\n",
    "https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/\n",
    "\n",
    "The *Hewlett ASAP* essays are already labeled as belonging to one of eight topics. A baseline excercise will determine how well essays are allocated to a topic using LDA.\n",
    "\n",
    "A second experiment will be performed using LDA to assign scores.\n",
    "\n",
    "Another important remark is that LDA is based on probability distributions. Probing these distributions introduces randomness so the results of running this notebook might not exactly match the comments or annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize confusion between the *LDA* derived topics and the *Hewlett ASAP* given topics, the given topic numbers will be replaced with a one-word summary.\n",
    "\n",
    "LDA uses the probability of finding certain words associated with documents. Stop words will not be very helpful, for example, the word \"the\" is going to have a high probability across all topics. In order to refine the word list, we'll also use the lemma generated by SpaCy instead of the regular essay. As a reminder, the lemma were generated on language corrected essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace topic numbers with meaningful one-word summary:\n",
    "topic_dict = {'topic':{1: 'computer', \n",
    "                       2: 'censorship', \n",
    "                       3: 'cyclist', \n",
    "                       4: 'hibiscus', \n",
    "                       5: 'mood', \n",
    "                       6: 'dirigibles', \n",
    "                       7: 'patience', \n",
    "                       8: 'laughter'}}\n",
    "\n",
    "training_set.replace(topic_dict, inplace=True)\n",
    "\n",
    "# Lemmatized essays re-joined (list to essay)\n",
    "training_set['l_essay'] = training_set['lemma'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert essays to a matrix of token (lemma) counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: number of unique lemma\n",
    "vectorizer = CountVectorizer(max_df=.2, \n",
    "                             min_df=3, \n",
    "                             stop_words=STOP_WORDS, \n",
    "                             max_features=2000) # default: binary=False\n",
    "doc_term_matrix = vectorizer.fit_transform(training_set.l_essay) # using lemmatized essays\n",
    "\n",
    "# Most frequent tokens:\n",
    "words = vectorizer.get_feature_names()\n",
    "doc_term_matrix_df = pd.SparseDataFrame(doc_term_matrix, columns=words)\n",
    "word_freq = doc_term_matrix_df.sum(axis=0).astype(int)\n",
    "word_freq.sort_values(ascending=False).head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LDA on the word frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_base = LatentDirichletAllocation(n_components=8,\n",
    "                                     n_jobs=-1,\n",
    "                                     learning_method='batch',\n",
    "                                     max_iter=40,\n",
    "                                     perp_tol=0.01,\n",
    "                                     verbose=1,\n",
    "                                     evaluate_every=5)\n",
    "lda_base.fit(doc_term_matrix)\n",
    "\n",
    "# save base model\n",
    "joblib.dump(lda_base, 'lda_baseline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic probabilities for all words. The numbered topics are generated from the latent Dirichlet allocation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = ['Topic {}'.format(i) for i in range(1, 9)]\n",
    "topics_count = lda_base.components_\n",
    "topics_prob = topics_count / topics_count.sum(axis=1).reshape(-1, 1)\n",
    "topics = pd.DataFrame(topics_prob.T,\n",
    "                      index=words,\n",
    "                      columns=topic_labels)\n",
    "topics.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word = list(topic_dict['topic'].values())\n",
    "sns.heatmap(topics.reindex(one_word), cmap='Blues')\n",
    "plt.title('Topic probabilities for one-word-summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap suggests assignments for all but 1 topics. \n",
    "\n",
    "Below are the most probable words for each topic. We can already see our one-word summaries of the actual topic near the top of the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = {}\n",
    "for topic, words_ in topics.items():\n",
    "    top_words[topic] = words_.nlargest(10).index.tolist()\n",
    "pd.DataFrame(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assign LDA topic probabilities to each essay and aggregate. It is now clear that, for example, LDA allocated topic 3 is aligned with the given topic \"computers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_preds = lda_base.transform(doc_term_matrix)\n",
    "train_eval = pd.DataFrame(train_preds, columns=topic_labels, index=training_set.topic)\n",
    "train_eval.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval.groupby(level='topic').mean().plot.bar(title='Avg. Topic Probabilities', rot=0, colormap='tab10', figsize=(10,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model was successful in that each given topic is allocated with high probability to an LDA topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_eval.groupby(level='topic').agg('median')\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "g = sns.heatmap(df, annot=True, fmt='.1%', annot_kws={\"size\": 10}, cmap='Blues', square=True)\n",
    "loc, labels = plt.yticks()\n",
    "g.set_yticklabels(labels, rotation=0)\n",
    "g.set_title('Topic Assignments');\n",
    "\n",
    "df = train_eval\\\n",
    "            .idxmax(axis=1)\\\n",
    "            .reset_index()\\\n",
    "            .groupby('topic', as_index=False)\\\n",
    "            .agg(lambda x:x.value_counts().index[0])\\\n",
    "            .rename(columns={0:'assignment'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with PyLDAVis\n",
    "\n",
    "#### Lambda\n",
    "\n",
    "- **$\\lambda$ = 0**: how probable is a word to appear in a topic - words are ranked on lift P(word | topic) / P(word)\n",
    "- **$\\lambda$ = 1**: how exclusive is a word to a topic -  words are purely ranked on P(word | topic)\n",
    "\n",
    "The ranking formula is $\\lambda * P(\\text{word} \\vert \\text{topic}) + (1 - \\lambda) * \\text{lift}$\n",
    "\n",
    "User studies suggest $\\lambda = 0.6$ works for most people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prepare(lda_base, doc_term_matrix, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Score Allocation\"\n",
    "\n",
    "Can we take this to the next level and assign target scores based on word probabilities? To keep it simple, we'll limit the essays to topic number 4, \"hibiscus\". This topic has only four target scores and a reasonably balanced distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hibiscus = training_set[training_set.topic == 'hibiscus']\n",
    "\n",
    "# Split essays into training and test sets\n",
    "train_essays, test_essays = train_test_split(hibiscus, \n",
    "                                         stratify=hibiscus.target_score, \n",
    "                                         test_size=0.2, \n",
    "                                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=.2, \n",
    "                             min_df=3, \n",
    "                             stop_words=STOP_WORDS, \n",
    "                             max_features=400) # limit to account for smaller set of essays\n",
    "\n",
    "# Train and test doc-term matrices\n",
    "train_dtm = vectorizer.fit_transform(train_essays.l_essay)\n",
    "test_dtm = vectorizer.fit_transform(test_essays.l_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_score = LatentDirichletAllocation(n_components=4,\n",
    "                                     n_jobs=-1,\n",
    "                                     learning_method='batch',\n",
    "                                     evaluate_every=5,\n",
    "                                     verbose=1, \n",
    "                                     max_iter=500)\n",
    "lda_score.fit(train_dtm)\n",
    "\n",
    "# save training model\n",
    "joblib.dump(lda_score, 'lda_score.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = ['Score A', 'Score B', 'Score C', 'Score D'] #.format(i) for i in range(1, 5)]\n",
    "\n",
    "train_preds = lda_score.transform(train_dtm)\n",
    "train_eval = pd.DataFrame(train_preds, columns=topic_labels, index=train_essays.target_score)\n",
    "train_eval.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_eval.groupby(level='target_score')\\\n",
    "            .mean()\\\n",
    "            .plot\\\n",
    "            .bar(title='Avg. Topic Probabilities', rot=0, colormap='tab10', figsize=(8,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = train_eval.groupby(level='target_score').agg('median')\n",
    "sns.heatmap(df, annot=True, fmt='.1%', cmap='Blues', square=True)\n",
    "plt.title('Train Data: Score Assignments');\n",
    "\n",
    "df = train_eval\\\n",
    "            .idxmax(axis=1)\\\n",
    "            .reset_index()\\\n",
    "            .groupby('target_score', as_index=False)\\\n",
    "            .agg(lambda x:x.value_counts().index[0])\\\n",
    "            .rename(columns={0:'assignment'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = lda_score.transform(test_dtm)\n",
    "test_eval = pd.DataFrame(test_preds, columns=topic_labels, index=test_essays.target_score)\n",
    "test_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval.groupby(level='target_score')\\\n",
    "            .mean()\\\n",
    "            .plot\\\n",
    "            .bar(title='Avg. Topic Probabilities', rot=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_eval.groupby(level='target_score').agg('median')\n",
    "sns.heatmap(df, annot=True, fmt='.1%', cmap='Blues', square=True)\n",
    "plt.title('Topic Assignments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(8,6), sharey=True)\n",
    "source = ['Train', 'Test']\n",
    "for i, df in enumerate([train_eval, test_eval]):\n",
    "    df = df.groupby(level='target_score').agg('median')\n",
    "    sns.heatmap(df, annot=True, fmt='.1%', cmap='Blues', square=True, ax=axes[i], cbar=False)\n",
    "    axes[i].set_title('{} Data: Topic Assignments'.format(source[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the charts above are very similar to a confusion matrix, the ordering of the LDA derived topics (A,B,C,D) doesn't necessarily match the human-labeled topic ordering (0,1,2,3). Thus, the high percentages, shown here as deep blues, are not expected to be found along the diagonal. Instead the goal is to find topic distinction, indicated by a single dark square in each column, and model accuracy, indicated by identical color patterns between train and test data sets.\n",
    "\n",
    "As seen above, there is some agreement between train and test data that essays with highest and lowest scores are distinct and assigned \"Score A\" and \"Score B\" respectively.  Overall, both topic distinction and model accuracy are rather poor. It is highly improbable this approach could be extended to any of the other topics due to the larger range of scores and class imbalance. \n",
    "\n",
    "Furthermore, repeated LDA runs show a lack of reproducability, which is a sign of poor distinction of topics. In summary, topic modelling, or more specifically using word frequencies and probabilities is not a useful tool to grade student essays. In the next notebook we'll continue with machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
